{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification, AutoConfig\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os \n",
        "import time\n",
        "import re\n",
        "import seaborn as sbn\n",
        "from string import ascii_lowercase\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T02:28:23.913296Z",
          "iopub.execute_input": "2023-04-17T02:28:23.913904Z",
          "iopub.status.idle": "2023-04-17T02:28:37.259908Z",
          "shell.execute_reply.started": "2023-04-17T02:28:23.913867Z",
          "shell.execute_reply": "2023-04-17T02:28:37.258695Z"
        },
        "trusted": true,
        "id": "ekOqeJosjHKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean(data):\n",
        "    data = data.lower() \n",
        "#     data = re.sub('j','i',data) \n",
        "    with open('../input/datacomments/teencode.txt','r') as file:\n",
        "      file = file.read()\n",
        "      lines = file.split('\\n')\n",
        "      for line in lines:\n",
        "        elements = line.split('\\t')\n",
        "        data = re.sub(r'\\b{}+\\b'.format(elements[0]), elements[1], data)\n",
        "    alphabet = 'abcdefghijlmnopqrstuvwxyz'\n",
        "    for c in alphabet:\n",
        "      data = re.sub(r'{}+'.format(c), c, data)\n",
        "\n",
        "    data = re.sub(r'\\s+', ' ', data)\n",
        "    return data\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T02:28:37.265763Z",
          "iopub.execute_input": "2023-04-17T02:28:37.268698Z",
          "iopub.status.idle": "2023-04-17T02:28:37.280160Z",
          "shell.execute_reply.started": "2023-04-17T02:28:37.268655Z",
          "shell.execute_reply": "2023-04-17T02:28:37.279040Z"
        },
        "trusted": true,
        "id": "bEWbFMHWjHKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# call the function\n",
        "df_train = pd.read_excel('../input/datacomments/train.xlsx')\n",
        "df_test =  pd.read_excel('../input/datacomments/test.xlsx')\n",
        "df_valid = pd.read_excel('../input/datacomments/valid.xlsx')\n",
        "\n",
        "df_train['Sentence'] = df_train['Sentence'].apply(clean)\n",
        "df_test['Sentence'] = df_test['Sentence'].apply(clean)\n",
        "df_valid['Sentence'] = df_valid['Sentence'].apply(clean)\n",
        "\n",
        "\n",
        "\n",
        "test_texts = list(df_test['Sentence'])\n",
        "train_texts = list(df_train['Sentence'])\n",
        "valid_texts = list(df_valid['Sentence'])\n",
        "\n",
        "y= LabelEncoder()\n",
        "\n",
        "train_labels = y.fit_transform(df_train['Emotion'])\n",
        "valid_labels = y.fit_transform(df_valid['Emotion'])\n",
        "test_labels = y.fit_transform(df_test['Emotion'])\n",
        "\n",
        "target_names = list(df_train.Emotion.unique())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T02:28:37.281625Z",
          "iopub.execute_input": "2023-04-17T02:28:37.282257Z",
          "iopub.status.idle": "2023-04-17T02:28:52.154283Z",
          "shell.execute_reply.started": "2023-04-17T02:28:37.282222Z",
          "shell.execute_reply": "2023-04-17T02:28:52.153211Z"
        },
        "trusted": true,
        "id": "fsfh6KInjHKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the model we gonna train, base uncased BERT\n",
        "# check text classification models here: https://huggingface.co/models?filter=text-classification\n",
        " \n",
        "# model_name = \"xlm-roberta-base\" #66 65 66 256 66 65 66 512\n",
        "# \n",
        "model_name = \"vinai/phobert-base\" #62 59 52 256 62 59 62 512\n",
        "\n",
        "\n",
        "\n",
        "# max sequence length for each document/sentence sample\n",
        "max_length = 512\n",
        "# load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T02:28:52.156884Z",
          "iopub.execute_input": "2023-04-17T02:28:52.157438Z",
          "iopub.status.idle": "2023-04-17T02:28:53.825737Z",
          "shell.execute_reply.started": "2023-04-17T02:28:52.157409Z",
          "shell.execute_reply": "2023-04-17T02:28:53.824745Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "f06aa26ecf334fe6a7145de861255d9b",
            "2171a4a8c41143bdb5ec8c2826ce7423",
            "74bcb96ddeaf4cbfbe7b292bc9201d29"
          ]
        },
        "id": "3PgwOKe0jHKN",
        "outputId": "446c3cc1-454a-4192-ee2f-83b180e21cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f06aa26ecf334fe6a7145de861255d9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2171a4a8c41143bdb5ec8c2826ce7423"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)solve/main/bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74bcb96ddeaf4cbfbe7b292bc9201d29"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the dataset, truncate when passed `max_length`, \n",
        "# and pad with 0's when less than `max_length`\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "\n",
        "# https://huggingface.co/transformers/v3.4.0/custom_datasets.html\n",
        "class NewsGroupsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# convert our tokenized data into a torch Dataset\n",
        "\n",
        "train_dataset = NewsGroupsDataset(train_encodings, train_labels)\n",
        "valid_dataset = NewsGroupsDataset(valid_encodings, valid_labels)\n",
        "test_dataset = NewsGroupsDataset(test_encodings, test_labels)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T02:28:53.827191Z",
          "iopub.execute_input": "2023-04-17T02:28:53.827668Z",
          "iopub.status.idle": "2023-04-17T02:28:54.812705Z",
          "shell.execute_reply.started": "2023-04-17T02:28:53.827629Z",
          "shell.execute_reply": "2023-04-17T02:28:54.811706Z"
        },
        "trusted": true,
        "id": "u63mw4wWjHKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "label2id = {\"Anger\": 0, \"Disgust\": 1, \"Enjoyment\": 2, \"Fear\": 3, \"Other\": 4, \"Sadness\": 5, \"Surprise\": 6}\n",
        "id2label = {0: \"Anger\", 1: \"Disgust\", 2: \"Enjoyment\", 3: \"Fear\", 4: \"Other\", 5: \"Sadness\", 6: \"Surprise\"}\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,label2id=label2id,\n",
        "                        id2label=id2label, num_labels=7)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T02:28:54.814241Z",
          "iopub.execute_input": "2023-04-17T02:28:54.814637Z",
          "iopub.status.idle": "2023-04-17T02:28:59.360424Z",
          "shell.execute_reply.started": "2023-04-17T02:28:54.814598Z",
          "shell.execute_reply": "2023-04-17T02:28:59.359476Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "3ab6ee07bef5445f8c447fc0fb506c0a"
          ]
        },
        "id": "X9Mo-RRxjHKR",
        "outputId": "579f6ca0-248b-4c78-df4f-824b21218170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/543M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ab6ee07bef5445f8c447fc0fb506c0a"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    # calculate accuracy using sklearn's function\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "      'accuracy': acc,\n",
        "    }\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='my_model',          # output directory\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy='epoch',\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=True,\n",
        "    optim=\"adamw_hf\",\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.1,               # strength of weight decay\n",
        "    learning_rate=5e-5,\n",
        "    gradient_accumulation_steps=1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps = round(len(train_dataset) / 16),\n",
        "    save_total_limit = 2\n",
        "\n",
        ")\n",
        "data_collator_ = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=valid_dataset,          # evaluation dataset\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interestm,\n",
        "    data_collator = data_collator_,\n",
        "    # optimizers=(optimizer, lr_scheduler)\n",
        "    # callbacks=[early_stopping]\n",
        ")\n",
        "# train the model\n",
        "time_start = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "\n",
        "# evaluate the current model after training\n",
        "\n",
        "time_end = time.time()\n",
        "total_time = time_end - time_start\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T02:28:59.361953Z",
          "iopub.execute_input": "2023-04-17T02:28:59.362420Z",
          "iopub.status.idle": "2023-04-17T02:38:24.732906Z",
          "shell.execute_reply.started": "2023-04-17T02:28:59.362381Z",
          "shell.execute_reply": "2023-04-17T02:38:24.731932Z"
        },
        "trusted": true,
        "id": "ADtye4M3jHKT",
        "outputId": "b89df084-4265-4f17-f2c7-db934ea38d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 5548\n  Num Epochs = 5\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1735\n  Number of trainable parameters = 135003655\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1735' max='1735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1735/1735 09:16, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.634100</td>\n      <td>1.232731</td>\n      <td>0.552478</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.098800</td>\n      <td>1.063544</td>\n      <td>0.597668</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.729100</td>\n      <td>1.061392</td>\n      <td>0.638484</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.435700</td>\n      <td>1.172383</td>\n      <td>0.641399</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.235400</td>\n      <td>1.348892</td>\n      <td>0.625364</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "***** Running Evaluation *****\n  Num examples = 686\n  Batch size = 16\nSaving model checkpoint to my_model/checkpoint-347\nConfiguration saved in my_model/checkpoint-347/config.json\nModel weights saved in my_model/checkpoint-347/pytorch_model.bin\ntokenizer config file saved in my_model/checkpoint-347/tokenizer_config.json\nSpecial tokens file saved in my_model/checkpoint-347/special_tokens_map.json\nadded tokens file saved in my_model/checkpoint-347/added_tokens.json\n***** Running Evaluation *****\n  Num examples = 686\n  Batch size = 16\nSaving model checkpoint to my_model/checkpoint-694\nConfiguration saved in my_model/checkpoint-694/config.json\nModel weights saved in my_model/checkpoint-694/pytorch_model.bin\ntokenizer config file saved in my_model/checkpoint-694/tokenizer_config.json\nSpecial tokens file saved in my_model/checkpoint-694/special_tokens_map.json\nadded tokens file saved in my_model/checkpoint-694/added_tokens.json\n***** Running Evaluation *****\n  Num examples = 686\n  Batch size = 16\nSaving model checkpoint to my_model/checkpoint-1041\nConfiguration saved in my_model/checkpoint-1041/config.json\nModel weights saved in my_model/checkpoint-1041/pytorch_model.bin\ntokenizer config file saved in my_model/checkpoint-1041/tokenizer_config.json\nSpecial tokens file saved in my_model/checkpoint-1041/special_tokens_map.json\nadded tokens file saved in my_model/checkpoint-1041/added_tokens.json\nDeleting older checkpoint [my_model/checkpoint-347] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 686\n  Batch size = 16\nSaving model checkpoint to my_model/checkpoint-1388\nConfiguration saved in my_model/checkpoint-1388/config.json\nModel weights saved in my_model/checkpoint-1388/pytorch_model.bin\ntokenizer config file saved in my_model/checkpoint-1388/tokenizer_config.json\nSpecial tokens file saved in my_model/checkpoint-1388/special_tokens_map.json\nadded tokens file saved in my_model/checkpoint-1388/added_tokens.json\nDeleting older checkpoint [my_model/checkpoint-694] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 686\n  Batch size = 16\nSaving model checkpoint to my_model/checkpoint-1735\nConfiguration saved in my_model/checkpoint-1735/config.json\nModel weights saved in my_model/checkpoint-1735/pytorch_model.bin\ntokenizer config file saved in my_model/checkpoint-1735/tokenizer_config.json\nSpecial tokens file saved in my_model/checkpoint-1735/special_tokens_map.json\nadded tokens file saved in my_model/checkpoint-1735/added_tokens.json\nDeleting older checkpoint [my_model/checkpoint-1041] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from my_model/checkpoint-1388 (score: 0.641399416909621).\n***** Running Evaluation *****\n  Num examples = 686\n  Batch size = 16\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'eval_loss': 1.172383189201355,\n 'eval_accuracy': 0.641399416909621,\n 'eval_runtime': 1.9142,\n 'eval_samples_per_second': 358.375,\n 'eval_steps_per_second': 22.464,\n 'epoch': 5.0}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "pred = trainer.predict(test_dataset)\n",
        "\n",
        "# print(test_labels)\n",
        "y_pred = np.argmax(pred.predictions, axis=1)\n",
        "# print(y_pred)\n",
        "print(classification_report(test_labels,y_pred, digits=3))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T02:38:24.734162Z",
          "iopub.execute_input": "2023-04-17T02:38:24.734765Z",
          "iopub.status.idle": "2023-04-17T02:38:26.770623Z",
          "shell.execute_reply.started": "2023-04-17T02:38:24.734726Z",
          "shell.execute_reply": "2023-04-17T02:38:26.769702Z"
        },
        "trusted": true,
        "id": "OtdC5qENjHKU",
        "outputId": "6ca082db-21e5-4b16-fd29-4df6f67e6f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "***** Running Prediction *****\n  Num examples = 693\n  Batch size = 16\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "              precision    recall  f1-score   support\n\n           0      0.442     0.575     0.500        40\n           1      0.612     0.538     0.573       132\n           2      0.761     0.710     0.735       193\n           3      0.717     0.826     0.768        46\n           4      0.549     0.612     0.579       129\n           5      0.706     0.724     0.715       116\n           6      0.793     0.622     0.697        37\n\n    accuracy                          0.657       693\n   macro avg      0.654     0.658     0.652       693\nweighted avg      0.664     0.657     0.658       693\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('transformers-phobert')\n",
        "# tokenizer.save_pretrained(\"tokenizer-bart\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T02:38:26.872245Z",
          "iopub.execute_input": "2023-04-17T02:38:26.872944Z",
          "iopub.status.idle": "2023-04-17T02:38:27.701592Z",
          "shell.execute_reply.started": "2023-04-17T02:38:26.872906Z",
          "shell.execute_reply": "2023-04-17T02:38:27.700608Z"
        },
        "trusted": true,
        "id": "DDWdfPDQjHKY",
        "outputId": "20fe0525-bd30-4176-dae5-7e9f912247d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Saving model checkpoint to trainer-phobert\nConfiguration saved in trainer-phobert/config.json\nModel weights saved in trainer-phobert/pytorch_model.bin\ntokenizer config file saved in trainer-phobert/tokenizer_config.json\nSpecial tokens file saved in trainer-phobert/special_tokens_map.json\nadded tokens file saved in trainer-phobert/added_tokens.json\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}